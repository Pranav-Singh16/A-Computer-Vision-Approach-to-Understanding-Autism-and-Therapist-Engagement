{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1o-7SL-llUdAHQB_8zkhkYIvImsP9rKAE","authorship_tag":"ABX9TyPo2mTKxaD2iJyXy2YNwlxv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"sTkZeohLmAd1","executionInfo":{"status":"ok","timestamp":1712847155921,"user_tz":-330,"elapsed":9092,"user":{"displayName":"Pranav Singh","userId":"14921887481779284305"}}},"outputs":[],"source":["import sys\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import signal\n","import cv2\n","from PIL import Image\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import torchvision.transforms as transforms"]},{"cell_type":"code","source":["frames = []\n","\n","def extract_frames(video_path, interval_seconds):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(\"Error: Could not open video file:\", video_path)\n","        return []\n","\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    interval_frames = int(fps * interval_seconds)\n","    frame_count = 0\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        frame_count += 1\n","        if frame_count % interval_frames == 0:\n","            # making frames gray and images sharper\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            blurred = cv2.GaussianBlur(gray, (5,5), 0)\n","            edges = cv2.Canny(blurred, 30, 80)\n","            frames.append(edges)\n","\n","    cap.release()\n","\n","video_path = '/content/drive/MyDrive/Internshala/Videos.mp4'\n","interval_seconds = 1  # we are taking 1 frame at every second\n","\n","extract_frames(video_path, interval_seconds)\n","enhanced_frames = np.array(frames)\n","total_number_of_frames = len(enhanced_frames)\n","\n","# Convert your data to PyTorch tensors\n","frames_predictions = torch.tensor(enhanced_frames)\n"],"metadata":{"id":"l1ofph4omOWK","executionInfo":{"status":"ok","timestamp":1712847821075,"user_tz":-330,"elapsed":3976,"user":{"displayName":"Pranav Singh","userId":"14921887481779284305"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Define the custom model class\n","class CustomModel(nn.Module):\n","    def __init__(self):\n","        super(CustomModel, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        self.output_branch1 = nn.Sequential(\n","            nn.Linear(256 * 11 * 20, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 3)\n","        )\n","\n","        self.output_branch2 = nn.Sequential(\n","            nn.Linear(256 * 11 * 20, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 3)\n","        )\n","\n","        self.output_branch3 = nn.Sequential(\n","            nn.Linear(256 * 11 * 20, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 3)\n","        )\n","\n","        self.output_branch4 = nn.Sequential(\n","            nn.Linear(256 * 11 * 20, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 3)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = torch.flatten(x, 1)\n","        out1 = self.output_branch1(x)\n","        out2 = self.output_branch2(x)\n","        out3 = self.output_branch3(x)\n","        out4 = self.output_branch4(x)\n","        return out1, out2, out3, out4\n","\n","# Assuming you have input images (frames_predictions)\n","images_tensor = torch.tensor(frames_predictions).float().unsqueeze(1)  # Convert grayscale images to PyTorch tensor and make it float\n","\n","# Define the dataset and data loader for prediction\n","val_dataset = TensorDataset(images_tensor)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Load the trained model\n","model = CustomModel()\n","\n","# give location of weights and bias\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/contweights.pth'))\n","model.eval()  # Set the model to evaluation mode\n","\n","# Make predictions on the test dataset\n","predictions = []\n","with torch.no_grad():\n","    for batch_data in val_loader:\n","        # Get inputs from the current batch\n","        inputs = batch_data[0]\n","\n","        # Forward pass\n","        outputs1, outputs2, outputs3, outputs4 = model(inputs)\n","        preds1 = torch.argmax(outputs1, dim=1)\n","        preds2 = torch.argmax(outputs2, dim=1)\n","        preds3 = torch.argmax(outputs3, dim=1)\n","        preds4 = torch.argmax(outputs4, dim=1)\n","        batch_preds = torch.stack((preds1, preds2, preds3, preds4), dim=1)\n","        predictions.append(batch_preds)\n","\n","# Convert predictions to a numpy array\n","predictions = torch.cat(predictions).numpy()\n","\n","# adding all frames\n","video_result = np.sum(predictions, axis=0)/total_number_of_frames\n","print(video_result)\n","# # Display predictions\n","# print(predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mll8_jJpn2p_","executionInfo":{"status":"ok","timestamp":1712848011568,"user_tz":-330,"elapsed":19602,"user":{"displayName":"Pranav Singh","userId":"14921887481779284305"}},"outputId":"6ed5c5e1-63b2-4bb0-aec7-cd85ea99b462"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-9efb890f2dea>:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  images_tensor = torch.tensor(frames_predictions).float().unsqueeze(1)  # Convert grayscale images to PyTorch tensor and make it float\n"]},{"output_type":"stream","name":"stdout","text":["[1.48192771 1.75903614 1.62650602 1.74698795]\n"]}]},{"cell_type":"code","source":["# labeling them\n","if video_result[0]<0.5:\n","  child_gaze = 'high'\n","elif video_result[0]<1.5:\n","  child_gaze = 'moderate'\n","else:\n","  child_gaze = 'low'\n","\n","if video_result[1]<0.5:\n","  theripist_gaze = 'high'\n","elif video_result[1]<1.5:\n","  theripist_gaze = 'moderate'\n","else:\n","  theripist_gaze = 'low'\n","\n","if video_result[2]<0.5:\n","  object_interaction = 'ball'\n","elif video_result[2]<1.5:\n","  object_interaction = 'puzzle'\n","else:\n","  object_interaction = 'nor ball neighther puzzle something else'\n","\n","if video_result[3]<0.5:\n","  engagement_level = 'high'\n","elif video_result[3]<1.5:\n","  engagement_level = 'moderate'\n","else:\n","  engagement_level = 'low'\n","\n","print(f'child gaze is with therpist is {child_gaze} therpist gaze is with child is {theripist_gaze} \\n the object with they interacting is {object_interaction} then engaement level betwwen them is {engagement_level}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UR0XdUuq0nh","executionInfo":{"status":"ok","timestamp":1712848812843,"user_tz":-330,"elapsed":25,"user":{"displayName":"Pranav Singh","userId":"14921887481779284305"}},"outputId":"4dd29b58-e35e-4b31-9925-fff4cdcc4a13"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["child gaze is with therpist is moderate therpist gaze is with child is low \n"," the object with they interacting is nor ball neighther puzzle something else then engaement level betwwen them is low\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"XUSYuEb6uX-f"}}]}